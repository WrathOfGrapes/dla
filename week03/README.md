# Week 03

### Prerequisites

* This lecture assumes you are familiar with the __attention mechanism__. If you're not feeling confident about this
  topic, we recommend that you read one of the following materials:
    * [How Attention works in Deep Learning: understanding the attention mechanism in sequence models](https://theaisummer.com/attention/)
        * Simple language. Easy to understand. Quite detailed. Not too technical. Surface knowledge.
    * [Sequence to Sequence (seq2seq) and Attention](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)
        * Nicely illustrated. Detailed math explanations. A bit bulk.

### Practice & homework

__TBA__

### Additional Materials

__TBA__

